# Техническое видение: LLM Telegram Bot

## 1. Технологии

### Основной стек
- **Язык программирования:** Python 3.11+
- **Telegram Bot Framework:** aiogram
- **LLM Provider:** OpenRouter через OpenAI client
- **Тестирование:** pytest
- **Управление зависимостями:** uv + pyproject.toml
- **Деплой:** Docker
- **Автоматизация:** Makefile

### Хранение данных
- **База данных:** Не используется
- **История диалогов:** Python dict/list в памяти
- **Конфигурация:** Гибридный подход (.env + config.py + system_prompt.md)

### Принципы выбора технологий
- Максимальная простота (KISS)
- Быстрое прототипирование
- Минимальные зависимости
- Простота развертывания

---

## 2. Принцип разработки

### Методология
- **Итеративная разработка** с быстрым MVP
- Разработка по одной функции за раз
- Тестирование каждой функции сразу после реализации
- Никаких сложных процессов - только необходимое

### Архитектурный подход
- **Функциональное программирование** - только функции, без ООП
- Простые функции с четкой ответственностью
- Минимум абстракций и классов
- Избегаем наследования и сложных паттернов

### Цикл разработки
1. Реализация функции
2. Локальное тестирование
3. Commit + Push
4. Деплой (при готовности к тестированию)

### Принципы кода
- **KISS** - Keep It Simple, Stupid
- **DRY** - Don't Repeat Yourself (только когда действительно нужно)
- **YAGNI** - You Aren't Gonna Need It
- Читаемость кода важнее "умных" решений
- Функции вместо классов

### Подход к тестированию
- Модульные тесты для критичной логики
- Ручное тестирование через Telegram для UX
- Никаких сложных integration тестов на старте

### Организация работы
- Все в одном репозитории
- Простая структура файлов
- Минимум зависимостей между модулями

---

## 3. Структура проекта

### Организация файлов
```
llm-tg-bot-start/
├── main.py                # Главный файл бота (точка входа)
├── src/                   # Исходный код
│   ├── config.py          # Конфигурация
│   ├── handlers.py        # Обработчики сообщений
│   ├── llm_client.py      # Работа с LLM
│   └── system_prompt.md   # Системный промпт о компании
├── tests/                 # Тесты
│   ├── test_handlers.py
│   └── test_llm_client.py
├── logs/                  # Файлы логов
│   └── llm_bot.log        # Основной лог файл
├── doc/                   # Документация
│   ├── product_idea.md
│   └── vision.md
├── pyproject.toml        # Конфигурация uv, метаданные и зависимости проекта
├── uv.lock               # Файл блокировки точных версий зависимостей (в git!)
├── .env                   # Переменные окружения (НЕ в git!)
├── env.example            # Пример переменных окружения
├── Dockerfile             # Контейнеризация
├── docker-compose.yml     # Оркестрация контейнеров
├── Makefile              # Автоматизация
├── README.md             # Описание проекта
└── .gitignore            # Игнорируемые файлы
```

### Принципы организации
- **Плоская структура** - main.py в корне для простого запуска
- **Разделение ответственности** - логика в src/, конфиги в корне
- **Минимум папок** - только src/, tests/, doc/
- **Простота импортов** - все модули легко доступны

### Назначение файлов
- `main.py` - точка входа, инициализация и запуск бота
- `src/config.py` - загрузка настроек из переменных окружения
- `src/handlers.py` - функции обработки сообщений от пользователей  
- `src/llm_client.py` - функции взаимодействия с OpenRouter

### Преимущества выбранной структуры
- Простой запуск: `python main.py`
- Легкая настройка Docker
- Соответствие принципу KISS
- Масштабируемость при необходимости

---

## 4. Архитектура проекта

### Схема взаимодействия
```
Пользователь → Telegram → aiogram → handlers.py → llm_client.py → OpenRouter → Ответ
```

### Основные компоненты

**🤖 main.py (Инициализация)**
- Загружает конфигурацию
- Инициализирует бота aiogram
- Регистрирует обработчики команд
- Запускает polling

**⚡ handlers.py (Обработка сообщений)**
- `handle_start()` - приветствие и инструкции
- `handle_help()` - справка по командам
- `handle_clear()` - очистка истории диалога
- `handle_message()` - основная логика диалога
- `handle_error()` - обработка ошибок

**🧠 llm_client.py (Работа с LLM)**
- `send_to_llm()` - отправка запроса в OpenRouter
- `build_prompt()` - формирование промпта с контекстом и системным сообщением
- `parse_response()` - обработка ответа
- `load_system_prompt()` - загрузка системного промпта из файла

**⚙️ config.py (Конфигурация)**
- `load_config()` - загрузка переменных окружения (через python-dotenv)
- Валидация обязательных настроек
- Настройки LLM (модель, температура, etc.)

**📝 system_prompt.md (Системный промпт)**
- Описание компании и услуг
- Инструкции для LLM по ведению диалога
- Примеры ответов и стиль общения

### Поток данных
1. Пользователь отправляет сообщение или команду
2. aiogram передает в соответствующий handler
3. Для обычных сообщений: формируется контекст диалога (dict/list в памяти)
4. `llm_client.py` загружает системный промпт и строит полный промпт
5. Отправляем запрос в OpenRouter
6. Возвращаем ответ пользователю
7. Сохраняем историю в памяти

### Управление состоянием
- **История диалогов:** словарь `{chat_id: [messages]}` в памяти
- **Системный промпт:** загружается один раз при старте
- **Ограничение истории:** максимум последних 20 сообщений на чат

### Команды бота
- `/start` - приветствие и начало работы
- `/help` - справка по использованию
- `/clear` - очистка истории диалога
- Обычные сообщения - консультация через LLM

---

## 5. Модель данных

### Структуры данных в памяти

**📜 История диалогов**
```python
chat_conversations = {
    "chat_id_123": [
        {"role": "user", "content": "Привет", "timestamp": "2024-01-01T10:00:00"},
        {"role": "assistant", "content": "Здравствуйте! Как дела?", "timestamp": "2024-01-01T10:00:05"}
    ],
    "chat_id_456": [...]
}
```

**🔐 Секретные настройки (.env)**
```env
TELEGRAM_BOT_TOKEN=your_telegram_bot_token
OPENROUTER_API_KEY=your_openrouter_api_key
```

**⚙️ Публичные настройки (config.py + pyproject.toml)**

*config.py - настройки приложения:*
```python
# Настройки LLM
MODEL_NAME = "openai/gpt-4o-mini"
TEMPERATURE = 0.7
MAX_TOKENS = 1000

# Настройки истории
MAX_HISTORY_LENGTH = 20

# Системные настройки
SYSTEM_PROMPT_FILE = "src/system_prompt.md"
```

*pyproject.toml - метаданные и зависимости:*
```toml
[project]
name = "llm-tg-bot"
version = "1.0.0"
description = "LLM-powered Telegram bot for customer consultation"
requires-python = ">=3.11"
dependencies = [
    "aiogram>=3.4.1",
    "openai>=1.12.0", 
    "python-dotenv>=1.0.1"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0"
]

[tool.uv]
dev-dependencies = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0"
]
```

**🔄 Загруженная конфигурация**
```python
config = {
    # Из .env
    "telegram_token": "...",
    "openrouter_api_key": "...",
    # Из config.py
    "model_name": "openai/gpt-4o-mini",
    "max_history_length": 20,
    "system_prompt": "загруженный из файла текст"
}
```

### Принципы работы с данными
- **Простота**: только dict и list, никаких классов
- **Ограничения**: максимум 20 сообщений на чат в истории
- **Очистка**: функция `/clear` удаляет историю чата
- **Память**: все данные теряются при перезапуске (OK для MVP)
- **Безопасность**: секреты в .env, настройки в config.py, зависимости в pyproject.toml

### Управление историей
- Автоматическая обрезка при превышении 20 сообщений
- Хранение по chat_id (поддержка групповых чатов)
- Простая структура: role + content + timestamp

---

## 6. Работа с LLM

### Интеграция с OpenRouter

**🔌 Подключение**
- OpenAI client с базовым URL: `https://openrouter.ai/api/v1`
- API ключ из переменной окружения `OPENROUTER_API_KEY`
- Модель: `openai/gpt-4o-mini`

**📝 Структура сообщений**
```python
messages = [
    {
        "role": "system", 
        "content": "системный промпт из system_prompt.md"
    },
    {
        "role": "user", 
        "content": "Привет, расскажи о ваших услугах"
    },
    {
        "role": "assistant", 
        "content": "Здравствуйте! Наша компания предлагает..."
    },
    {
        "role": "user", 
        "content": "новое сообщение пользователя"
    }
]
```

### Основные функции

**🔄 build_prompt(chat_id, user_message)**
1. Загружаем системный промпт из `system_prompt.md`
2. Создаем сообщение с ролью `system`
3. Добавляем историю чата (последние 20 сообщений)
4. Добавляем новое сообщение пользователя
5. Возвращаем массив messages для OpenAI API

**🚀 send_to_llm(messages)**
1. Отправляем запрос в OpenRouter
2. Обрабатываем ответ
3. При ошибке возвращаем понятное сообщение (без retry)

### Настройки LLM
```python
# config.py
MODEL_NAME = "openai/gpt-4o-mini"
TEMPERATURE = 0.7        # баланс креативности/точности
MAX_TOKENS = 1000        # достаточно для консультации
SYSTEM_PROMPT_FILE = "src/system_prompt.md"
```

### Обработка ошибок (без retry)
- **Timeout**: "Извините, сервис временно недоступен. Попробуйте еще раз."
- **Rate limit**: "Слишком много запросов. Подождите немного и повторите."
- **API error**: "Произошла техническая ошибка. Обратитесь к администратору."
- **Общая ошибка**: "Не удалось получить ответ. Попробуйте переформулировать вопрос."

### Принципы работы
- **Простота**: никаких retry механизмов
- **Прозрачность**: понятные сообщения об ошибках
- **Контроль пользователя**: сам решает повторять запрос или нет
- **KISS**: минимальная логика обработки ошибок

---

## 7. Мониторинг LLM

### Логирование в файл

**📝 Что логируем**
- Запуск и останов бота
- Запросы к LLM с временными метками
- Ответы от LLM с временем выполнения
- Ошибки LLM с детальным описанием
- Chat ID для связи с пользователями

**📋 Формат логов**
```
2024-01-01 10:30:00 | BOT_START | version=1.0.0 | config_loaded=success
2024-01-01 10:30:15 | LLM_REQUEST | chat_id=123456 | model=gpt-4o-mini | message_length=45
2024-01-01 10:30:17 | LLM_RESPONSE | chat_id=123456 | response_time=2.1s | tokens_used=150
2024-01-01 10:30:20 | LLM_ERROR | chat_id=789012 | error=timeout | message="Request timeout after 30s"
2024-01-01 11:30:00 | BOT_STOP | uptime=1h | total_requests=147
```

**📂 Настройки логирования**
```python
# config.py
LOG_FILE = "logs/llm_bot.log"
LOG_LEVEL = "INFO"
LOG_FORMAT = "%(asctime)s | %(levelname)s | %(message)s"
LOG_TO_CONSOLE = True  # Дублировать логи в консоль
```

### Реализация

**⚙️ Настройка двойного вывода**
```python
# Настройка logger с FileHandler + StreamHandler
import logging
from logging.handlers import RotatingFileHandler

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Handler для файла (с ротацией)
file_handler = RotatingFileHandler("logs/llm_bot.log", maxBytes=10*1024*1024, backupCount=5)
file_handler.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))

# Handler для консоли
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))

logger.addHandler(file_handler)
logger.addHandler(console_handler)
```

**🔧 Функции логирования**
```python
def log_bot_start(version, config_status):
    logger.info(f"BOT_START | version={version} | config_loaded={config_status}")

def log_bot_stop(uptime, total_requests):
    logger.info(f"BOT_STOP | uptime={uptime} | total_requests={total_requests}")

def log_llm_request(chat_id, message_length, model):
    logger.info(f"LLM_REQUEST | chat_id={chat_id} | model={model} | message_length={message_length}")

def log_llm_response(chat_id, response_time, tokens_used):
    logger.info(f"LLM_RESPONSE | chat_id={chat_id} | response_time={response_time}s | tokens_used={tokens_used}")

def log_llm_error(chat_id, error_type, error_message):
    logger.error(f"LLM_ERROR | chat_id={chat_id} | error={error_type} | message=\"{error_message}\"")
```

### Принципы логирования
- **Двойной вывод**: одновременно в файл и консоль (через multiple handlers)
- **Структурированность**: единый формат для парсинга
- **Полнота**: достаточно информации для отладки
- **KISS**: стандартный Python logging, никаких внешних систем

### Ротация логов
- Автоматическая ротация при достижении 10MB
- Хранение последних 5 файлов
- Формат: `llm_bot.log`, `llm_bot.log.1`, etc.

---

## 8. Сценарии работы

### Основные пользовательские сценарии

**🚀 Первое знакомство**
1. Пользователь находит бота в Telegram
2. Отправляет `/start`
3. Получает приветствие и краткую информацию о возможностях
4. Видит примеры вопросов, которые можно задать

**💬 Типичная консультация**
1. Пользователь задает вопрос о услугах компании
2. Бот анализирует запрос через LLM с системным промптом
3. Предоставляет релевантную информацию из базы знаний
4. Задает уточняющие вопросы при необходимости
5. Предлагает конкретные услуги и следующие шаги

**🆘 Получение помощи**
1. Пользователь отправляет `/help`
2. Получает список доступных команд
3. Видит примеры полезных вопросов
4. Получает информацию о возможностях бота

**🔄 Очистка истории**
1. Пользователь отправляет `/clear`
2. История диалога для этого чата удаляется
3. Получает подтверждение успешной очистки
4. Может начать новый диалог "с чистого листа"

**⚠️ Обработка ошибок**
1. LLM недоступен - пользователь получает понятное сообщение
2. Предлагается повторить запрос позже
3. При критических ошибках - рекомендация обратиться к администратору
4. Бот продолжает работать для других функций

### Принципы взаимодействия
- **Понятность**: простые и ясные ответы
- **Полезность**: фокус на решении проблем пользователя
- **Естественность**: диалог как с живым консультантом
- **KISS**: никаких сложных меню и кнопок

---

## 9. Деплой

### Развертывание через Docker и Git

**🐳 Dockerfile**
```dockerfile
FROM python:3.11-slim

# Установка uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Рабочая директория
WORKDIR /app

# Копирование конфигурации зависимостей
COPY pyproject.toml uv.lock ./

# Установка зависимостей
RUN uv sync --frozen

# Копирование исходного кода
COPY . .

# Запуск приложения
CMD ["uv", "run", "python", "main.py"]
```

**🚀 docker-compose.yml**
- Один сервис для бота
- Монтирование логов в volume
- Переменные окружения из `.env` файла
- Автоматический перезапуск: `restart: unless-stopped`

### Процесс деплоя

**🔧 Локальное тестирование**
1. Клонирование репозитория: `git clone ...`
2. Установка зависимостей: `uv sync`
3. Создание `.env` на основе `env.example`
4. Заполнение `src/system_prompt.md`
5. Локальный запуск: `make run` или `uv run python main.py`
6. Тестирование через Telegram

**🌐 Деплой на VPS**
1. Подключение к VPS по SSH
2. Клонирование репозитория: `git clone ...`
3. Установка uv: `curl -LsSf https://astral.sh/uv/install.sh | sh`
4. Установка зависимостей: `uv sync`
5. Создание продуктивного `.env` на основе `env.example`
6. Настройка `system_prompt.md` для продакшена
7. Запуск: `make run`

**🔄 Обновление через Git**
1. Push изменений в репозиторий
2. На VPS: `git pull origin main`
3. Перезапуск: `make restart`

### Автоматизация через Makefile

**📋 Доступные команды**
```makefile
help:      # Показать список доступных команд
install:   # Установить зависимости через uv
dev:       # Запустить бота локально через uv
export:    # Экспортировать requirements.txt из pyproject.toml (если нужен для legacy систем)
build:     # Собрать Docker образ
run:       # Запустить бота в фоне (Docker)
stop:      # Остановить бота
restart:   # Перезапустить бота (stop + run)
logs:      # Показать логи бота
test:      # Запустить тесты через uv
clean:     # Очистить Docker образы и кеш uv
```

### Окружения

**💻 Локальная разработка**
- `.env` с тестовыми токенами
- Подключение к боту через @BotFather (тестовый бот)
- Логи одновременно в файл и консоль (удобно для отладки)

**🚀 Продуктивный VPS**
- `.env` с боевыми токенами
- Основной бот для пользователей
- Логи в файл с ротацией + консоль (для мониторинга)
- Мониторинг через `docker logs`

### Требования к VPS
- Ubuntu 20.04+ или аналог
- Docker и docker-compose
- Git
- Минимум 512MB RAM
- Постоянное подключение к интернету

---

## Заключение

### Готовность к разработке

Техническое видение **LLM Telegram Bot** полностью проработано и готово к реализации. Все ключевые решения приняты в соответствии с принципом **KISS (Keep It Simple, Stupid)**, что обеспечивает:

- 🚀 **Быстрый старт** - минимальное время от старта до первого рабочего MVP
- 🛠️ **Простота разработки** - функциональное программирование, понятная архитектура
- 📈 **Масштабируемость** - заложена возможность развития проекта
- 🔧 **Легкость поддержки** - простой деплой, понятное логирование

### Ключевые преимущества архитектуры

1. **Минимальный стек технологий** - только необходимое
2. **Современное управление зависимостями** - uv + pyproject.toml, один источник истины
3. **Отсутствие БД** - вся логика в памяти для MVP
4. **Контейнеризация** - простой деплой на любом VPS
5. **Двойное логирование** - удобство разработки и мониторинга
6. **Гибридная конфигурация** - безопасность + простота

### Следующие шаги

1. **Создание репозитория** и базовой структуры файлов
2. **Настройка окружения** с uv и pyproject.toml
3. **Создание .env** на основе env.example с токенами
4. **Разработка core функций** по модулям (config → llm_client → handlers → main)
5. **Создание system_prompt.md** с информацией о вашей компании
6. **Локальное тестирование** через тестового бота (`uv run python main.py`)
7. **Деплой на VPS** и финальное тестирование

### Оценка времени реализации

- **MVP (базовая функциональность)**: 3-5 дней
- **Полная версия с тестами**: 1-2 недели
- **Деплой и настройка продуктивной среды**: 1 день

**Проект готов к разработке! 🎯**

---

*Документ создан: январь 2024*  
*Статус: Готов к реализации*  
*Принцип: KISS - максимальная простота для MVP*
